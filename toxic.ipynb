{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install git+https://github.com/huggingface/optimum"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install git+https://github.com/huggingface/accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install transformers --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","from optimum.bettertransformer import BetterTransformer\n","from accelerate import Accelerator, notebook_launcher # main interface, distributed launcher\n","from accelerate.utils import set_seed # reproducability across devices\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","dir = None\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        dir = os.path.join(dirname, filename)\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["read = [\"comment_id\", \"comment_text\"]\n","df_comments = pd.read_csv(dir, usecols=read)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","import nltk\n","# download the NLTK resources (run this line only once)\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define a function to tag POS in a sentence\n","def pos_tag(sentence):\n","    tokens = nltk.word_tokenize(sentence)\n","    tags = nltk.pos_tag(tokens)\n","    return tags\n","\n","# apply the function to the 'text' column of the DataFrame\n","df_comments['pos_tags'] = df_comments['comment_text'].apply(pos_tag)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def is_question(tags):\n","    # check if the sentence starts with an auxiliary verb\n","    if len(tags) > 2 and tags[0][1].startswith('VB') and tags[1][1].startswith('PR'):\n","        # check if the third tag is a main verb\n","        return tags[2][1].startswith('VB')\n","    else:\n","        return False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["is_question_cond = ((df_comments.pos_tags.apply(lambda x: is_question(x))) | \n","                    (df_comments.comment_text.str.startswith((\"What\", \"When\", \"Where\", \"Which\", \"Who\", \"Whom\", \"Whose\", \"Why\", \"How\", \"Could\", \"Should\", \"Would\", \"Can\"))) | \n","                    (df_comments.comment_text.str.contains(\"?\", regex=False)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_comments[\"questions\"] = is_question_cond.astype(bool)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import transformers\n","\n","DOWNLOAD_URL = \"https://github.com/unitaryai/detoxify/releases/download/\"\n","MODEL_URLS = {\n","    \"original\": DOWNLOAD_URL + \"v0.1-alpha/toxic_original-c1212f89.ckpt\",\n","    \"unbiased\": DOWNLOAD_URL + \"v0.3-alpha/toxic_debiased-c7548aa0.ckpt\",\n","    \"multilingual\": DOWNLOAD_URL + \"v0.4-alpha/multilingual_debiased-0b549669.ckpt\",\n","    \"original-small\": DOWNLOAD_URL + \"v0.1.2/original-albert-0e1d6498.ckpt\",\n","    \"unbiased-small\": DOWNLOAD_URL + \"v0.1.2/unbiased-albert-c8519128.ckpt\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_model_and_tokenizer(\n","    model_type, model_name, tokenizer_name, num_classes, state_dict, huggingface_config_path=None\n","):\n","    model_class = getattr(transformers, model_name)\n","    model = model_class.from_pretrained(\n","        pretrained_model_name_or_path=None,\n","        config=huggingface_config_path or model_type,\n","        num_labels=num_classes,\n","        state_dict=state_dict,\n","        local_files_only=huggingface_config_path is not None,\n","    )\n","    tokenizer = getattr(transformers, tokenizer_name).from_pretrained(\n","        huggingface_config_path or model_type,\n","        local_files_only=huggingface_config_path is not None,\n","        # TODO: may be needed to let it work with Kaggle competition\n","        # model_max_length=512,\n","    )\n","\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_checkpoint(model_type=\"original\", checkpoint=None, device=\"cpu\", huggingface_config_path=None):\n","    if checkpoint is None:\n","        checkpoint_path = MODEL_URLS[model_type]\n","        loaded = torch.hub.load_state_dict_from_url(checkpoint_path, map_location=device)\n","    else:\n","        loaded = torch.load(checkpoint, map_location=device)\n","        if \"config\" not in loaded or \"state_dict\" not in loaded:\n","            raise ValueError(\n","                \"Checkpoint needs to contain the config it was trained \\\n","                    with as well as the state dict\"\n","            )\n","    class_names = loaded[\"config\"][\"dataset\"][\"args\"][\"classes\"]\n","    # standardise class names between models\n","    change_names = {\n","        \"toxic\": \"toxicity\",\n","        \"identity_hate\": \"identity_attack\",\n","        \"severe_toxic\": \"severe_toxicity\",\n","    }\n","    class_names = [change_names.get(cl, cl) for cl in class_names]\n","    model, tokenizer = get_model_and_tokenizer(\n","        **loaded[\"config\"][\"arch\"][\"args\"],\n","        state_dict=loaded[\"state_dict\"],\n","        huggingface_config_path=huggingface_config_path,\n","    )\n","\n","    return model, tokenizer, class_names"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model, tokenizer, class_names = load_checkpoint()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = BetterTransformer.transform(model, keep_original_model=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","# Report the number of sentences.\n","print('Number of test sentences: {:,}\\n'.format(df_comments.shape[0]))\n","\n","# Create sentence and label lists\n","sentences = df_comments.comment_text.values\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 512,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import TensorDataset, SequentialSampler, DataLoader\n","import pickle\n","# Create the DataLoader.\n","prediction_data = TensorDataset(input_ids, attention_masks)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Prediction on test set\n","def training_loop(model, dataloader, mixed_precision:str=\"fp16\", seed:int=42, batch_size:int=32):\n","\n","    \n","    accelerator = Accelerator()\n","    \n","    model = accelerator.prepare(model)\n","    \n","    # Put model on accelerator device\n","    model = model.to(accelerator.device)\n","\n","    # Wrap dataloader with accelerator.prepare()\n","    dataloader = accelerator.prepare(dataloader)\n","\n","    # Put model in evaluation mode\n","    model.eval()\n","    \n","    predictions=[]\n","    \n","    # Predict \n","    for batch in dataloader:\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask = batch\n","\n","        # Telling the model not to compute or store gradients, saving memory and \n","        # speeding up prediction\n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions\n","            outputs_toxic = model(b_input_ids, token_type_ids=None, \n","            attention_mask=b_input_mask)\n","\n","        logits_toxic = outputs_toxic[0]\n","\n","        # Store predictions and true labels\n","        predictions.append(torch.sigmoid(accelerator.gather(logits_toxic)).cpu().detach().numpy())\n","        \n","    pickle.dump(predictions, open(\"/kaggle/working/preds.pickle\", \"wb\"))\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","args = (model, prediction_dataloader, \"fp16\", 42, 32)\n","notebook_launcher(training_loop, args, num_processes=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions = pickle.load(open(\"/kaggle/working/preds.pickle\", \"rb\"))\n","flat_predictions = np.concatenate(predictions, axis=0)\n","name = \"negative\"\n","df_comments[class_names] = flat_predictions\n","\n","df_comments = df_comments[~df_comments.comment_text.str.startswith(\"@\")].dropna()\n","cond = (df_comments.toxicity > df_comments.toxicity.median()) & (df_comments.obscene < 0.15)  & (df_comments.insult > df_comments.insult.quantile(0.9)) & (df_comments.identity_attack > df_comments.identity_attack.quantile(0.99))\n","df_comments[\"negative\"] = cond\n","\n","df_comments.drop(class_names, 1).reset_index().to_csv(\"preds.csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
